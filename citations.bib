@article{Gao2020,
abstract = {Hand gesture recognition plays an important role in human–robot interaction. The accuracy and reliability of hand gesture recognition are the keys to gesture-based human–robot interaction tasks. To solve this problem, a method based on multimodal data fusion and multiscale parallel convolutional neural network (CNN) is proposed in this paper to improve the accuracy and reliability of hand gesture recognition. First of all, data fusion is conducted on the sEMG signal, the RGB image, and the depth image of hand gestures. Then, the fused images are generated to two different scale images by downsampling, which are respectively input into two subnetworks of the parallel CNN to obtain two hand gesture recognition results. After that, hand gesture recognition results of the parallel CNN are combined to obtain the final hand gesture recognition result. Finally, experiments are carried out on a self-made database containing 10 common hand gestures, which verify the effectiveness and superiority of the proposed method for hand gesture recognition. In addition, the proposed method is applied to a seven-degree-of-freedom bionic manipulator to achieve robotic manipulation with hand gestures.},
author = {Gao, Qing and Liu, Jinguo and Ju, Zhaojie},
doi = {10.1111/exsy.12490},
file = {:C$\backslash$:/Users/roume/Documents/GitHub/American-Sign-Language{\_}MNIST/Hand gesture recognition using multimodal data fusion and multiscale parallel convolutional neural network for human–robot interaction.pdf:pdf},
issn = {14680394},
journal = {Expert Systems},
keywords = {hand gesture recognition,multimodal data fusion,parallel CNN,sEMG signal},
number = {January},
title = {{Hand gesture recognition using multimodal data fusion and multiscale parallel convolutional neural network for human–robot interaction}},
year = {2020}
}
@article{Bantupalli2018,
author = {Bantupalli, Kshitij},
file = {:C$\backslash$:/Users/roume/Documents/GitHub/American-Sign-Language{\_}MNIST/American Sign Language Recognition Using Machine Learning and Computer Vision.pdf:pdf},
title = {{American Sign Language Recognition Using Machine Learning and Computer Vision}},
year = {2018}
}
@misc{Retana2019,
author = {Retana, David},
booktitle = {Towards Data Science},
title = {{A practical example to learn Transfer learning with PyTorch}},
url = {https://towardsdatascience.com/a-practical-example-in-transfer-learning-with-pytorch-846bb835f2db},
year = {2019}
}
@misc{Wan2020,
abstract = {In this tutorial, you'll use computer vision to build an American Sign Language translator for your webcam. As you work through the tutorial, you'll use OpenCV, a computer-vision library, PyTorch to build a deep neural network, and onnx to export your neural network. You'll also apply the following concepts as you build a computer-vision application:},
author = {Wan, Alvin},
booktitle = {Digital Ocean},
title = {{How To Build a Neural Network to Translate Sign Language into English}},
url = {https://www.digitalocean.com/community/tutorials/how-to-build-a-neural-network-to-translate-sign-language-into-english},
year = {2020}
}
@misc{Sarkar2018,
author = {Sarkar, Dipanjan (DJ)},
booktitle = {Towards Data Science},
title = {{A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning}},
url = {https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a},
year = {2018}
}
@misc{Koehrsen2018,
author = {Koehrsen, Will},
booktitle = {Towards Data Science},
title = {{Transfer Learning with Convolutional Neural Networks in PyTorch}},
url = {https://towardsdatascience.com/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce},
year = {2018}
}
@article{Mitchell2005,
abstract = {This study traces the sources of the estimates of how many people use American Sign Language (ASL) in the United States. A variety of claims can be found in the literature and on the Internet, some of which have been shown to be unfounded but continue to be cited. In our search for the sources of the various (mis)understandings, we have found that all data-based estimates of the number of people who use ASL in the United States have their origin in a single study published in the early 1970s, which inquired about signing in general and not ASL use in particular. There has been neither subsequent research to update these estimates of the prevalence of signing nor any specific study of ASL use. The paper concludes with a call to action to rectify this problem.},
author = {Mitchell, Ross E. and Young, Travas A. and Bachleda, Bellamie and Karchmer, Michael A.},
title = {{How Many People Use ASL in the United States?}},
url = {https://web.archive.org/web/20110604191021/https://www.ncdhhs.gov/mhddsas/deafservices/ASL{\_}Users.pdf},
year = {2005}
}
@misc{Akash2018,
abstract = {Image data set for alphabets in the American Sign Language},
author = {Akash},
booktitle = {Kaggle},
keywords = {health conditions,image data,image processing,language resources,linguistics,social networks,society},
mendeley-tags = {health conditions,image data,image processing,language resources,linguistics,social networks,society},
title = {{ASL Alphabet, Version 1}},
url = {https://www.kaggle.com/grassknoted/asl-alphabet},
urldate = {2020-01-06},
year = {2018}
}
@misc{Thakur2019,
abstract = {American Sign Language Dataset for Image Classifcation. This dataset can be used to apply the ideas of multi class classification using the technology of your choice. This is curated for convolution neural networks. The multi class classification result will be close to 98{\%} of accuracy if the algorithm is good enough.},
author = {Thakur, Ayush},
booktitle = {Kaggle},
keywords = {cnn,image data,image processing,mathematics,multiclass classification,reference,society},
mendeley-tags = {cnn,image data,image processing,mathematics,multiclass classification,reference,society},
title = {{American Sign Language Dataset, Version 1}},
url = {https://www.kaggle.com/ayuraj/asl-dataset/metadata},
urldate = {2020-01-06},
year = {2019}
}
@article{Gao2020a,
abstract = {At present, vision-based hand gesture recognition is very important in human-robot interaction (HRI). This non-contact method enables natural and friendly interaction between people and robots. Aiming at this technology, a two-stream CNN framework (2S-CNN) is proposed to recognize the American sign language (ASL) hand gestures based on multimodal (RGB and depth) data fusion. Firstly, the hand gesture data is enhanced to remove the influence of background and noise. Secondly, hand gesture RGB and depth features are extracted for hand gesture recognition using CNNs on two streams, respectively. Finally, a fusion layer is designed for fusing the recognition results of the two streams. This method utilizes multimodal data to increase the recognition accuracy of the ASL hand gestures. The experiments prove that the recognition accuracy of 2S-CNN can reach 92.08 {\$}{\$}$\backslash${\%}{\$}{\$} on ASL fingerspelling database and is higher than that of baseline methods.},
author = {Gao, Qing and Ogenyi, Uchenna Emeoha and Liu, Jinguo and Ju, Zhaojie and Liu, Honghai},
doi = {10.1007/978-3-030-29933-0_9},
file = {:C$\backslash$:/Users/roume/Documents/GitHub/American-Sign-Language{\_}MNIST/A Two-Stream CNN Framework for American Sign Language Recognition Based on Multimodal Data Fusion.pdf:pdf},
isbn = {9783030299323},
issn = {21945365},
journal = {Advances in Intelligent Systems and Computing},
keywords = {CNN,Hand gesture recognition,Multimodal data fusion},
number = {January},
pages = {107--118},
title = {{A Two-Stream CNN Framework for American Sign Language Recognition Based on Multimodal Data Fusion}},
volume = {1043},
year = {2020}
}
@article{Starner1995,
abstract = {Hidden Markov models (HMM's) have been used prominently and successfully in speech recognition and, more recently, in handwriting recognition. Consequently, they seem ideal for visual recognition of complex, structured hand gestures such as are found in sign language. We describe a real-time HMM-based system for recognizing sentence level American Sign Language (ASL) which attains a word accuracy of 99.2{\%} without explicitly modeling the fingers.},
author = {Starner, Thad and Pentland, Alex},
doi = {10.1109/iscv.1995.477012},
file = {:C$\backslash$:/Users/roume/Documents/GitHub/American-Sign-Language{\_}MNIST/Real-time American Sign Language recognition from video using hidden Markov models.pdf:pdf},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {265--270},
title = {{Real-time American Sign Language recognition from video using Hidden Markov models}},
year = {1995}
}
@article{Starner2013,
author = {Starner, Thad and Pendland, Alex},
file = {:C$\backslash$:/Users/roume/Documents/GitHub/American-Sign-Language{\_}MNIST/Visual Recognition of American Sign Language Using Hidden Markov Models.pdf:pdf},
pages = {2013},
title = {{Visual Recognition of American Sign Language Using Hidden Markov Models}},
year = {2013}
}
@article{Sun2013,
abstract = {Sign language recognition is a growing research area in the field of computer vision. A challenge within it is to model various signs, varying with time resolution, visual manual appearance, and so on. In this paper, we propose a discriminative exemplar coding (DEC) approach, as well as utilizing Kinect sensor, to model various signs. The proposed DEC method can be summarized as three steps. First, a quantity of class-specific candidate exemplars are learned from sign language videos in each sign category by considering their discrimination. Then, every video of all signs is described as a set of similarities between frames within it and the candidate exemplars. Instead of simply using a heuristic distance measure, the similarities are decided by a set of exemplar-based classifiers through the multiple instance learning, in which a positive (or negative) video is treated as a positive (or negative) bag and those frames similar to the given exemplar in Euclidean space as instances. Finally, we formulate the selection of the most discriminative exemplars into a framework and simultaneously produce a sign video classifier to recognize sign. To evaluate our method, we collect an American sign language dataset, which includes approximately 2000 phrases, while each phrase is captured by Kinect sensor with color, depth, and skeleton information. Experimental results on our dataset demonstrate the feasibility and effectiveness of the proposed approach for sign language recognition. {\textcopyright} 2013 IEEE.},
author = {Sun, Chao and Zhang, Tianzhu and Bao, Bing Kun and Xu, Changsheng and Mei, Tao},
doi = {10.1109/TCYB.2013.2265337},
file = {:C$\backslash$:/Users/roume/Documents/GitHub/American-Sign-Language{\_}MNIST/Discriminative Exemplar Coding for Sign Language Recognition With Kinect.pdf:pdf},
issn = {21682267},
journal = {IEEE Transactions on Cybernetics},
keywords = {Discriminative exemplar coding,Kinect sensor,Sign language recognition},
number = {5},
pages = {1418--1428},
title = {{Discriminative exemplar coding for sign language recognition with kinect}},
volume = {43},
year = {2013}
}
@article{Sun2013a,
author = {Sun, Chao and Zhang, Tianzhu and Bao, Bing-kun and Xu, Changsheng},
file = {:C$\backslash$:/Users/roume/Documents/GitHub/American-Sign-Language{\_}MNIST/Latent support vector machine for sign language recognition with Kinect.pdf:pdf},
isbn = {9781479923410},
journal = {Icip},
pages = {4190--4194},
title = {{LATENT SUPPORT VECTOR MACHINE FOR SIGN LANGUAGE RECOGNITION WITH KINECT National Laboratory of Pattern Recognition , Institute of Automation , Chinese Academy of Sciences , Beijing , China China-Singapore Institute of Digital Media , Singapore}},
year = {2013}
}
@article{Aryanie2015,
abstract = {This paper presents finger-spelling recognition method for American Sign Language (ASL) Alphabet using k-Nearest Neighbours (k-NN) Classifier. This research also examines the effect of PCA for dimensional reduction to k-NN performance. The empiric results show that k-NN classifier achieves the highest accuracy (99.8 percent) for k=3 when the pattern is represented by full dimensional feature. However, k-NN classifier only achieves 28.6 percent accuracy (for k=5) when the pattern is represented by PCAreduced dimensional feature. This low accuracy is due to several factors, among others, is the presence of high numbers of redundant or highly correlated features among ASL alphabet that makes PCA unable to separate data. Although kNN classifier accuracy is higher than the proposed classifier in [7], recognition time of k-NN classifier is longer than that of the method proposed in [7]. Therefore, k-NN classifier is suitable for early child education-based application such as self-assessment system for special need student who learns ASL alphabet finger-spelling.},
author = {Aryanie, Dewinta and Heryadi, Yaya},
doi = {10.1109/ICoICT.2015.7231481},
file = {:C$\backslash$:/Users/roume/Documents/GitHub/American-Sign-Language{\_}MNIST/American sign language-based finger-spelling recognition using k-Nearest Neighbors classifier.pdf:pdf},
isbn = {9781479977529},
journal = {2015 3rd International Conference on Information and Communication Technology, ICoICT 2015},
keywords = {finger-spelling recognition},
pages = {533--536},
title = {{American sign language-based finger-spelling recognition using k-Nearest Neighbors classifier}},
year = {2015}
}
@article{Pigou2015,
abstract = {There is an undeniable communication problem between the Deaf community and the hearing majority. Innovations in automatic sign language recognition try to tear down this communication barrier. Our contribution considers a recognition system using the Microsoft Kinect, convolutional neural networks (CNNs) and GPU acceleration. Instead of constructing complex handcrafted features, CNNs are able to automate the process of feature construction. We are able to recognize 20 Italian gestures with high accuracy. The predictive model is able to generalize on users and surroundings not occurring during training with a cross-validation accuracy of 91.7{\%}. Our model achieves a mean Jaccard Index of 0.789 in the ChaLearn 2014 Looking at People gesture spotting competition.},
author = {Pigou, Lionel and Dieleman, Sander and Kindermans, Pieter-Jan and Schrauwen, Benjamin},
file = {:C$\backslash$:/Users/roume/Documents/GitHub/American-Sign-Language{\_}MNIST/Sign Language Recognition using Convolutional Neural Networks.pdf:pdf},
isbn = {978-3-319-16178-5},
keywords = {convolutional neural network,deep learning,gesture recog-,nition,sign language recognition},
pages = {572--578},
title = {{Sign Language Recognition Using Convolutional Neural Networks BT - Computer Vision - ECCV 2014 Workshops}},
url = {https://core.ac.uk/download/pdf/55693048.pdf},
year = {2015}
}
@article{Mocialov2018,
abstract = {Automatic speech recognition and spoken dialogue systems have made great advances through the use of deep machine learning methods. This is partly due to greater computing power but also through the large amount of data available in common languages, such as English. Conversely, research in minority languages, including sign languages, is hampered by the severe lack of data. This has led to work on transfer learning methods, whereby a model developed for one language is reused as the starting point for a model on a second language, which is less resourced. In this paper, we examine two transfer learning techniques of fine-tuning and layer substitution for language modelling of British Sign Language. Our results show improvement in perplexity when using transfer learning with standard stacked LSTM models, trained initially using a large corpus for standard English from the Penn Treebank corpus.},
author = {Mocialov, Boris and Hastie, Helen and Turner, Graham},
file = {:C$\backslash$:/Users/roume/Documents/GitHub/American-Sign-Language{\_}MNIST/Transfer Learning for British Sign Language Modelling.pdf:pdf},
journal = {Proceedings of the Fifth Workshop on {\{}NLP{\}} for Similar Languages, Varieties and Dialects ({\{}V{\}}ar{\{}D{\}}ial 2018)},
pages = {101--110},
title = {{Transfer Learning for British Sign Language Modelling}},
url = {http://www.bslcorpusproject.org/{\%}0Ahttps://www.aclweb.org/anthology/W18-3911},
year = {2018}
}
@article{Atwood2012,
abstract = {Sign language translation is a promising application for vision-based gesture recognition methods, in which highly- structured combinations of static and dynamic gestures correlate to a given lexicon. Machine learning techniques can be used to create interactive educational tools or to help a hearing-impaired person communicate more effectively with someone who does not know sign language. In this paper, the development of an online sign language recognizer is described. The scope of the project is limited to static letters in the American Sign Language (ASL) alphabet. Two machine learning approaches were implemented: (1) a single hidden layer neural network and (2) a principal component analysis (PCA) model. In the former case, images were processed to reduce the number of pixels (input nodes to the network) while maintaining an appropriate amount of variance between signs. Over-fitting was avoided using k-fold cross validation (k=2). The PCA model facilitated reduced dimensionality without loss of relevant information (e.g. from scaling or normalization). The results indicate that both approaches recognize signs effectively for subjects included in the training process ({\textgreater}95{\%}), while untrained subjects produce poor accuracy ({\~{}}40-70{\%}). When all subjects were included in the training set, the best neural network exhibited 95.8{\%} accuracy compared to 96.1{\%} accuracy for the PCA model. Custom MATLAB user interfaces were created for acquiring training samples and for testing the machine learning approaches on live data streamed from a webcam. Despite high error for unseen subjects in offline processing, the system is able to recognize all letters in the real- time GUI simply by adjusting the hand position or orientation. Future improvements include incorporating a dynamic bounding box, lifting the restrictions on scaling/ rotation/background noise, and recognition of dynamic letters and two-handed words. INTRODUCTION},
author = {Atwood, Jason and Eicholtz, MAtthew and Farrell, Justin},
file = {:C$\backslash$:/Users/roume/Documents/GitHub/American-Sign-Language{\_}MNIST/American Sign Language Recognition System.pdf:pdf},
pages = {1--4},
title = {{American Sign Language Recognition System}},
url = {https://jasonatwood.io/wp-content/uploads/2012/12/Atwood-Eicholtz-and-Farrell-ASL-Recognition.pdf},
year = {2012}
}
@article{Garcia2016,
abstract = {A real-time sign language translator is an important milestone in facilitating communication between the deaf community and the general public. We hereby present the development and implementation of an American Sign Language (ASL) fingerspelling translator based on a convolutional neural network. We utilize a pre-trained GoogLeNet architecture trained on the ILSVRC2012 dataset, as well as the Surrey University and Massey University ASL datasets in order to apply transfer learning to this task. We produced a robust model that consistently classifies letters a-e correctly with first-time users and another that correctly classifies letters a-k in a majority of cases. Given the limitations of the datasets and the encouraging results achieved, we are confident that with further research and more data, we can produce a fully generalizable translator for all ASL letters.},
author = {Garcia, Brandon and Viesca, Sigberto Alarcon},
file = {:C$\backslash$:/Users/roume/Documents/GitHub/American-Sign-Language{\_}MNIST/Real-time American Sign Language Recognition with Convolutional Neural.pdf:pdf},
journal = {Stanford CS231n},
keywords = {American Sign Language,gesture recognition},
title = {{Real-time American Sign Language Recognition with Convolutional Neural Networks}},
url = {http://cs231n.stanford.edu/reports/2016/pdfs/214{\_}Report.pdf},
year = {2016}
}
