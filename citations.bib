@misc{Akash2018,
abstract = {Image data set for alphabets in the American Sign Language},
author = {Akash},
booktitle = {Kaggle},
keywords = {health conditions,image data,image processing,language resources,linguistics,social networks,society},
mendeley-tags = {health conditions,image data,image processing,language resources,linguistics,social networks,society},
title = {{ASL Alphabet, Version 1}},
url = {https://www.kaggle.com/grassknoted/asl-alphabet},
urldate = {2020-01-06},
year = {2018}
}
@misc{Thakur2019,
abstract = {American Sign Language Dataset for Image Classifcation. This dataset can be used to apply the ideas of multi class classification using the technology of your choice. This is curated for convolution neural networks. The multi class classification result will be close to 98{\%} of accuracy if the algorithm is good enough.},
author = {Thakur, Ayush},
booktitle = {Kaggle},
keywords = {cnn,image data,image processing,mathematics,multiclass classification,reference,society},
mendeley-tags = {cnn,image data,image processing,mathematics,multiclass classification,reference,society},
title = {{American Sign Language Dataset, Version 1}},
url = {https://www.kaggle.com/ayuraj/asl-dataset/metadata},
urldate = {2020-01-06},
year = {2019}
}
@article{Garcia2016,
abstract = {A real-time sign language translator is an important milestone in facilitating communication between the deaf community and the general public. We hereby present the development and implementation of an American Sign Language (ASL) fingerspelling translator based on a convolutional neural network. We utilize a pre-trained GoogLeNet architecture trained on the ILSVRC2012 dataset, as well as the Surrey University and Massey University ASL datasets in order to apply transfer learning to this task. We produced a robust model that consistently classifies letters a-e correctly with first-time users and another that correctly classifies letters a-k in a majority of cases. Given the limitations of the datasets and the encouraging results achieved, we are confident that with further research and more data, we can produce a fully generalizable translator for all ASL letters.},
author = {Garcia, Brandon and Viesca, Sigberto Alarcon},
file = {:C$\backslash$:/Users/roume/Documents/GitHub/American-Sign-Language{\_}MNIST/Real-time American Sign Language Recognition with Convolutional Neural.pdf:pdf},
journal = {Stanford CS231n},
keywords = {American Sign Language,gesture recognition},
title = {{Real-time American Sign Language Recognition with Convolutional Neural Networks}},
url = {http://cs231n.stanford.edu/reports/2016/pdfs/214{\_}Report.pdf},
year = {2016}
}
@article{Mocialov2018,
abstract = {Automatic speech recognition and spoken dialogue systems have made great advances through the use of deep machine learning methods. This is partly due to greater computing power but also through the large amount of data available in common languages, such as English. Conversely, research in minority languages, including sign languages, is hampered by the severe lack of data. This has led to work on transfer learning methods, whereby a model developed for one language is reused as the starting point for a model on a second language, which is less resourced. In this paper, we examine two transfer learning techniques of fine-tuning and layer substitution for language modelling of British Sign Language. Our results show improvement in perplexity when using transfer learning with standard stacked LSTM models, trained initially using a large corpus for standard English from the Penn Treebank corpus.},
author = {Mocialov, Boris and Hastie, Helen and Turner, Graham},
file = {:C$\backslash$:/Users/roume/Documents/GitHub/American-Sign-Language{\_}MNIST/Transfer Learning for British Sign Language Modelling.pdf:pdf},
journal = {Proceedings of the Fifth Workshop on {\{}NLP{\}} for Similar Languages, Varieties and Dialects ({\{}V{\}}ar{\{}D{\}}ial 2018)},
pages = {101--110},
title = {{Transfer Learning for British Sign Language Modelling}},
url = {http://www.bslcorpusproject.org/{\%}0Ahttps://www.aclweb.org/anthology/W18-3911},
year = {2018}
}
@misc{Koehrsen2018,
author = {Koehrsen, Will},
booktitle = {Towards Data Science},
title = {{Transfer Learning with Convolutional Neural Networks in PyTorch}},
url = {https://towardsdatascience.com/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce},
year = {2018}
}
@article{Mitchell2005,
abstract = {This study traces the sources of the estimates of how many people use American Sign Language (ASL) in the United States. A variety of claims can be found in the literature and on the Internet, some of which have been shown to be unfounded but continue to be cited. In our search for the sources of the various (mis)understandings, we have found that all data-based estimates of the number of people who use ASL in the United States have their origin in a single study published in the early 1970s, which inquired about signing in general and not ASL use in particular. There has been neither subsequent research to update these estimates of the prevalence of signing nor any specific study of ASL use. The paper concludes with a call to action to rectify this problem.},
author = {Mitchell, Ross E. and Young, Travas A. and Bachleda, Bellamie and Karchmer, Michael A.},
title = {{How Many People Use ASL in the United States?}},
url = {https://web.archive.org/web/20110604191021/https://www.ncdhhs.gov/mhddsas/deafservices/ASL{\_}Users.pdf},
year = {2005}
}
@article{Atwood2012,
abstract = {Sign language translation is a promising application for vision-based gesture recognition methods, in which highly- structured combinations of static and dynamic gestures correlate to a given lexicon. Machine learning techniques can be used to create interactive educational tools or to help a hearing-impaired person communicate more effectively with someone who does not know sign language. In this paper, the development of an online sign language recognizer is described. The scope of the project is limited to static letters in the American Sign Language (ASL) alphabet. Two machine learning approaches were implemented: (1) a single hidden layer neural network and (2) a principal component analysis (PCA) model. In the former case, images were processed to reduce the number of pixels (input nodes to the network) while maintaining an appropriate amount of variance between signs. Over-fitting was avoided using k-fold cross validation (k=2). The PCA model facilitated reduced dimensionality without loss of relevant information (e.g. from scaling or normalization). The results indicate that both approaches recognize signs effectively for subjects included in the training process ({\textgreater}95{\%}), while untrained subjects produce poor accuracy ({\~{}}40-70{\%}). When all subjects were included in the training set, the best neural network exhibited 95.8{\%} accuracy compared to 96.1{\%} accuracy for the PCA model. Custom MATLAB user interfaces were created for acquiring training samples and for testing the machine learning approaches on live data streamed from a webcam. Despite high error for unseen subjects in offline processing, the system is able to recognize all letters in the real- time GUI simply by adjusting the hand position or orientation. Future improvements include incorporating a dynamic bounding box, lifting the restrictions on scaling/ rotation/background noise, and recognition of dynamic letters and two-handed words. INTRODUCTION},
author = {Atwood, Jason and Eicholtz, MAtthew and Farrell, Justin},
file = {:C$\backslash$:/Users/roume/Documents/GitHub/American-Sign-Language{\_}MNIST/American Sign Language Recognition System.pdf:pdf},
pages = {1--4},
title = {{American Sign Language Recognition System}},
url = {https://jasonatwood.io/wp-content/uploads/2012/12/Atwood-Eicholtz-and-Farrell-ASL-Recognition.pdf},
year = {2012}
}
@article{Pigou2015,
abstract = {There is an undeniable communication problem between the Deaf community and the hearing majority. Innovations in automatic sign language recognition try to tear down this communication barrier. Our contribution considers a recognition system using the Microsoft Kinect, convolutional neural networks (CNNs) and GPU acceleration. Instead of constructing complex handcrafted features, CNNs are able to automate the process of feature construction. We are able to recognize 20 Italian gestures with high accuracy. The predictive model is able to generalize on users and surroundings not occurring during training with a cross-validation accuracy of 91.7{\%}. Our model achieves a mean Jaccard Index of 0.789 in the ChaLearn 2014 Looking at People gesture spotting competition.},
author = {Pigou, Lionel and Dieleman, Sander and Kindermans, Pieter-Jan and Schrauwen, Benjamin},
file = {:C$\backslash$:/Users/roume/Documents/GitHub/American-Sign-Language{\_}MNIST/Sign Language Recognition using Convolutional Neural Networks.pdf:pdf},
isbn = {978-3-319-16178-5},
keywords = {convolutional neural network,deep learning,gesture recog-,nition,sign language recognition},
pages = {572--578},
title = {{Sign Language Recognition Using Convolutional Neural Networks BT - Computer Vision - ECCV 2014 Workshops}},
url = {https://core.ac.uk/download/pdf/55693048.pdf},
year = {2015}
}
@misc{Sarkar2018,
author = {Sarkar, Dipanjan (DJ)},
booktitle = {Towards Data Science},
title = {{A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning}},
url = {https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a},
year = {2018}
}
@misc{Retana2019,
author = {Retana, David},
booktitle = {Towards Data Science},
title = {{A practical example to learn Transfer learning with PyTorch}},
url = {https://towardsdatascience.com/a-practical-example-in-transfer-learning-with-pytorch-846bb835f2db},
year = {2019}
}
@misc{Wan2020,
abstract = {In this tutorial, you'll use computer vision to build an American Sign Language translator for your webcam. As you work through the tutorial, you'll use OpenCV, a computer-vision library, PyTorch to build a deep neural network, and onnx to export your neural network. You'll also apply the following concepts as you build a computer-vision application:},
author = {Wan, Alvin},
booktitle = {Digital Ocean},
title = {{How To Build a Neural Network to Translate Sign Language into English}},
url = {https://www.digitalocean.com/community/tutorials/how-to-build-a-neural-network-to-translate-sign-language-into-english},
year = {2020}
}
